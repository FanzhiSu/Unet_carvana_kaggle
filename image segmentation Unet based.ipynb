{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80d6ec9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1918, 1280)\n",
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "#check image shape\n",
    "from PIL import Image\n",
    "\n",
    "im = Image.open('C:/Users/clarq/Desktop/data/train/1aba91a601c6_01.jpg')\n",
    "\n",
    "print(im.size)\n",
    "print(type(im.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a8afaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create convolutional net\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e450a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1,stride=1), #two 3x3 conv\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv(inputs)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        return x\n",
    "# if __name__=='__main__':\n",
    "#     inputs=torch.rand((2,3,512,512))\n",
    "#     c=DoubleConv(3,64) #build a class called DoubleConv, the class has input 3 and output 64\n",
    "#     c(inputs)   # , and this is used on the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8986c76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = DoubleConv(in_ch, out_ch)\n",
    "        self.pool = nn.MaxPool2d(kernel_size =2,stride=2)# kernel size is 2x2 for maxpool2d\n",
    "        \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv(inputs)\n",
    "        p = self.pool(x)\n",
    "       # print('conv',x.shape)\n",
    "        #print('pool',p.shape)\n",
    "\n",
    "        return x,p\n",
    "# if __name__=='__main__':\n",
    "#     inputs=torch.rand((2,3,512,512))\n",
    "#     c=Down(3,64)\n",
    "#     c(inputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6fed7f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoder\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, bilinear=False):\n",
    "        super(Up, self).__init__()\n",
    "\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_ch , out_ch , kernel_size=2, stride=2,padding=0) \n",
    "        self.conv=DoubleConv(out_ch+out_ch , out_ch)# the input channel size of this conv net is the concatenation \n",
    "                                                    # channel size of the skip connection\n",
    "                                                    # and the output of the decoder\n",
    "\n",
    "    def forward(self, inputs, skip):#inputs and skip connections\n",
    "        x_upsample = self.up(inputs)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # the skip connection is larger than the spsampling output\n",
    "        # so, (1)either the skip connection input needs to be cropped, or (2) the upsampled layer is padded to the size\n",
    "        # of the skip connection (method (2)is used here)\n",
    "        \n",
    "        if x_upsample.shape != skip.shape:\n",
    "            diffY = skip.size()[2]  - x_upsample.size()[2] # presumably, the skip connection is larger than the upsample\n",
    "                                                       # diffY is the height difference between the two images, \n",
    "                                                       # skip.size()[2], [2] is the second element in skip.size()\n",
    "                                                       # [a,b,c,d] a:batch size; b:channel; c:height; d:width\n",
    "       \n",
    "        \n",
    "        \n",
    "        \n",
    "            diffX = skip.size()[3] - x_upsample.size()[3] \n",
    "            # presumably, the skip connection is larger than the upsample\n",
    "            # diffX is the width difference between the two images\n",
    "        \n",
    "            x_padded = F.pad(x_upsample, (diffX // 2, diffX - diffX // 2,diffY // 2, diffY - diffY // 2),value=0)\n",
    "            # zero padding,'//' means divide and take the whole value of the output only refer to \n",
    "            # onenote 'PhD lit - explain F.pad'\n",
    "        else:\n",
    "            x_padded=x_upsample\n",
    "        \n",
    "        \n",
    "        x_concatenate = torch.cat([x_padded,skip], dim=1)# concatenates the channels (dim=1, i.e., the second \n",
    "                                       # element in the shape bracket) of the skip and the output of the decoder(up)\n",
    "        x = self.conv(x_concatenate)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# if __name__=='__main__':\n",
    "#     inputs=torch.rand((2,64,256,256))\n",
    "#     skip=torch.rand((2,32,512,512))# this should the size of the image in the previous layer\n",
    "#     c=Up(64,32)\n",
    "#     print(c(inputs,skip).shape)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7fc59409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output of the unet with convolutional net (traditional CNN) no relu in the final 1X1 covolutional\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size= 1, padding=0)# when kernel_size=1, padding always 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70ef6f86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s1 torch.Size([2, 64, 520, 520]) p1 torch.Size([2, 64, 260, 260])\n",
      "s2 torch.Size([2, 128, 260, 260]) p2 torch.Size([2, 128, 130, 130])\n",
      "s3 torch.Size([2, 256, 130, 130]) p3 torch.Size([2, 256, 65, 65])\n",
      "s4 torch.Size([2, 512, 65, 65]) p4 torch.Size([2, 512, 32, 32])\n",
      "torch.Size([2, 1024, 32, 32])\n",
      "y torch.Size([2, 1, 520, 520])\n"
     ]
    }
   ],
   "source": [
    "#start to build the whole Unet architecture\n",
    "#change the input feature at self.down1 \n",
    "#change the output feature(classes) at self.outc\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "#         self.n_channels = in_channels\n",
    "#         self.n_classes =  classes\n",
    "\n",
    "        '''Input'''\n",
    "       # self.inc = InConv(1, 64)\n",
    "        \n",
    "        '''Encoder'''\n",
    "        self.down1 = Down(3, 64)#encoder layer1,1 is the input feature channel,  you can change         \n",
    "        self.down2 = Down(64, 128)#encoder layer2\n",
    "        self.down3 = Down(128, 256)#encoder layer3\n",
    "        self.down4 = Down(256, 512)#encoder layer4\n",
    "        \n",
    "        '''Bottleneck'''\n",
    "        self.bottleneck = DoubleConv(512, 1024) #bottleneck, no skip connection, the bottom of the 'U-shape'\n",
    "        \n",
    "        '''Decoder'''\n",
    "        self.up1 = Up(1024, 512)#decoder layer1\n",
    "        self.up2 = Up(512,256)#decoder layer2\n",
    "        self.up3 = Up(256, 128)#decoder layer3\n",
    "        self.up4 = Up(128, 64)#decoder layer4\n",
    "        \n",
    "        \n",
    "        '''classifier'''\n",
    "        self.outc = OutConv(64, 1)# the '3' is the number of classes. if binary classification, it is 1\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        s1,p1 = self.down1(inputs)# s is the output dimensions after the skip connection concatenated\n",
    "                                  # p is the output dimension from the maxpooling layer\n",
    "        s2,p2 = self.down2(p1)\n",
    "        s3,p3 = self.down3(p2)\n",
    "        s4,p4 = self.down4(p3)\n",
    "        print('s1',s1.shape,'p1',p1.shape)\n",
    "        print('s2',s2.shape,'p2',p2.shape)\n",
    "        print('s3',s3.shape,'p3',p3.shape)\n",
    "        print('s4',s4.shape,'p4',p4.shape)\n",
    "        b = self.bottleneck(p4)\n",
    "        print(b.shape)\n",
    "        d1 = self.up1(b, s4)\n",
    "        d2 = self.up2(d1, s3)\n",
    "        d3 = self.up3(d2, s2)\n",
    "        d4 = self.up4(d3, s1)\n",
    "        output = self.outc(d4)\n",
    "        #print('d4',d4.shape)\n",
    "        return output\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    inputs=torch.rand((2,3,520,520))\n",
    "    model=Unet()\n",
    "    y=model(inputs)\n",
    "    print('y',y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69047d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test():\n",
    "#     x=torch.rand((3,1,160,160))\n",
    "#     model=Unet()\n",
    "#     y=model(x)\n",
    "#     print('y',y.shape)\n",
    "#     print('x',x.shape)\n",
    "#     assert y.shape == x.shape\n",
    "\n",
    "# if __name__=='__main__':    \n",
    "#     test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1f6fbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader \n",
    "import os\n",
    "from PIL import Image \n",
    "# pillow library\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "#dataset class,this is classic\n",
    "class T_Dataset(Dataset):\n",
    "    def __init__(self,image_directory,mask_directory,transform=True):\n",
    "        # image_directory is the image path,transform is augmentations,\n",
    "        # mask_directory can be changed to targets if it is image classification\n",
    "        self.image_directory = image_directory\n",
    "        self.mask_directory = mask_directory\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(image_directory)\n",
    "        #os.listdir() method in python is used to get the list of all files and directories\n",
    "        #in the specified directory. If we don’t specify any directory, \n",
    "        #then list of files and directories in the current working directory will be returned.\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)# size of image dataset \n",
    "    \n",
    "    def __getitem__(self,index): # index iterates from 0 to number of dataset\n",
    "        img_path=os.path.join(self.image_directory, self.images[index])\n",
    "        #os.path.join() method in Python join one or more path components intelligently\n",
    "        mask_path=os.path.join(self.mask_directory, self.images[index].replace('.jpg','_mask.gif'))\n",
    "                                                                             # in this mask，\n",
    "                                                                             # white is labeled as 255.0 \n",
    "                                                                             #and black is labeled as 0.0 \n",
    "                \n",
    "        mask[mask == 255.0] =1.0 # preprocess for the mask, since sigmoid activation is used, so transfer 255 to 1\n",
    "        \n",
    "        \n",
    "        #image=np.transpose(image,(2,0,1).astype(np.float32)) # change the array of image to channel first\n",
    "        #tensor.unsqueeze(0) #if the iamge is greyscale, you need to add one dimension to it\n",
    "        if self.transform is not None:# data augmentaion\n",
    "            augmentations =self.transform(image = image, mask = mask)\n",
    "            image = augmentations['image']\n",
    "            mask = augmentations ['mask']\n",
    "        return image,mask\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fac13c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training part\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm #loading progress bar\n",
    "import torch.optim as optim\n",
    "\n",
    "#save & load mode\n",
    "def save_checkpoint(state,filename='my_checkpoint.pth.tar'):\n",
    "    print('=> saving checkpoint')\n",
    "    torch.save(state,filename)\n",
    "\n",
    "def load_checkpoint(checkpoint,model):\n",
    "    print('=> loading checkpoint')\n",
    "    model.load.state.dict(checkpoint['state_dict'])\n",
    "\n",
    "def check_accuracy(loader,model,device='cuda'):\n",
    "    num_correct=0\n",
    "    num_pixels=0\n",
    "    dice_score=0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for image,mask in loader:\n",
    "            images=image.to(device)\n",
    "            masks=mask.to(device).unsqueeze(1) # since this is greyscale so add one dimension with unsqueeze\n",
    "            preds=torch.sigmoid(model(images))\n",
    "            preds=(preds>0.5).float()\n",
    "            num_correct+=(preds==masks).sum()\n",
    "            num_pixels+=torch.numel(preds)#numel: number of elements\n",
    "            dice_score+= (2*(preds*masks).sum())/((preds+masks).sum()+1e-8) \n",
    "            # this is for binary to evaluate the output, google for multiclass\n",
    "    print(f' got{num_correct}/{num_pixels} with acc {num_correct/num_pixels*100:.2f}' ) \n",
    "    print(f' Dice score:{dice_score/len(loader)}' )\n",
    "    model.train()\n",
    "\n",
    "def save_predictions_as_imgs(loader,model,folder='C:/Users/clarq/Desktop/data/saved images',device='cuda'):\n",
    "    model.eval()\n",
    "    for idx,(images,masks) in enumerate(loader):\n",
    "        images=images.to(device=Device)\n",
    "        with torch.no_grad():\n",
    "            preds=torch.sigmoid(model(images))\n",
    "            preds=(preds>0.5).float()\n",
    "        torchvision.utils.save_image(\n",
    "            preds,f'{folder}/pred_{idx}.png')\n",
    "        torchvision.utils.save_image(masks.unsqueeze(1),f'{folder}/true_{idx}.png')\n",
    "        \n",
    "    model.train() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed557f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "learning_rate=1e-4\n",
    "Device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "Batch_size=16\n",
    "num_epochs=3\n",
    "num_workers=0\n",
    "Image_height=160  # 1280 originally, use 160 in this example(only use a small part of the image),\n",
    "                # in real competition, use 1280\n",
    "Image_width=240 #1918 originally\n",
    "Pin_memory= True\n",
    "Load_model= False\n",
    "val_percent: float = 0.3\n",
    "train_image_directory='C:/Users/clarq/Desktop/data/train'\n",
    "train_mask_directory='C:/Users/clarq/Desktop/data/train_masks'\n",
    "# val_img_directory='C:/Users/clarq/Desktop/data/val'\n",
    "# val_mask_directory='C:/Users/clarq/Desktop/data/val_masks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9831b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " got42117941/53376000 with acc 78.91\n",
      " Dice score:0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 203/203 [02:46<00:00,  1.22it/s, loss=0.158]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> saving checkpoint\n",
      " got52639141/53376000 with acc 98.62\n",
      " Dice score:0.9666340351104736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 203/203 [02:34<00:00,  1.31it/s, loss=0.115]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> saving checkpoint\n",
      " got52863339/53376000 with acc 99.04\n",
      " Dice score:0.9771881103515625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 203/203 [02:30<00:00,  1.35it/s, loss=0.0789]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> saving checkpoint\n",
      " got52955507/53376000 with acc 99.21\n",
      " Dice score:0.9813230037689209\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "#data augmentation,create model\n",
    "from torch.utils.data import DataLoader\n",
    "def main():\n",
    "    train_transform = A.Compose([\n",
    "        A.Resize(height=Image_height, width= Image_width),\n",
    "        A.Rotate(limit=35,p=1.0),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.1),\n",
    "        A.Normalize(mean=[0,0,0],std=[1,1,1],max_pixel_value=225),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "#     val_transform = A.Compose([\n",
    "#         A.Resize(height=Image_height, width= Image_width),\n",
    "#         A.Normalize(mean=[0,0,0],std=[1,1,1],max_pixel_value=225),\n",
    "#         ToTensorV2(),\n",
    "#     ])\n",
    "    \n",
    "\n",
    "\n",
    "    model=Unet().to(Device)\n",
    "    loss_function=nn.BCEWithLogitsLoss()# bianry cross entropy, sigmoid is included in the loss function\n",
    "                                         # use CROSS entropy loss for multiple classification\n",
    "\n",
    "    optimizer= optim.Adam(model.parameters(),lr=learning_rate)\n",
    "    \n",
    "\n",
    "    ds=T_Dataset(image_directory=train_image_directory,mask_directory=train_mask_directory,\n",
    "                     transform=train_transform)\n",
    "\n",
    "    \n",
    "    n_val = int(len(ds) * val_percent)\n",
    "    n_train = len(ds) - n_val\n",
    "    train_set, val_set = torch.utils.data.random_split(ds, [n_train, n_val],\n",
    "                                                   generator=torch.Generator().manual_seed(0))\n",
    "    train_loader = DataLoader(train_set, shuffle=True,batch_size=Batch_size,pin_memory=Pin_memory,num_workers=0)\n",
    "    val_loader = DataLoader(val_set, shuffle=False, batch_size=Batch_size,pin_memory=Pin_memory,num_workers=0)\n",
    "      \n",
    "    \n",
    "    if Load_model: #Load model = False as set, change to True to use this if loop\n",
    "        load_checkpoint(torch.load('my_checkpoint.pth.tar'),model)\n",
    "        \n",
    "    check_accuracy(val_loader,model,device=Device)\n",
    "        \n",
    "        \n",
    "        \n",
    "    scaler=torch.cuda.amp.GradScaler()\n",
    "    \n",
    "  \n",
    "    for epoch in range(num_epochs):\n",
    "        loop=tqdm(train_loader)    \n",
    "        for batch_idx, (data,targets) in enumerate(loop):\n",
    "            data= data.to(device=Device)# data is the input image\n",
    "            targets=targets.float().unsqueeze(1).to(device=Device)#target is the masks\n",
    "            #forward\n",
    "            with torch.cuda.amp.autocast():\n",
    "                predictions=model(data)\n",
    "                loss=loss_function(predictions,targets)\n",
    "            \n",
    "        \n",
    "        \n",
    "            #backward\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        \n",
    "            #update tadm loop\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "         \n",
    "        #save model\n",
    "        checkpoint={'state_dict': model.state_dict(),'optimizer':optimizer.state_dict(),}\n",
    "        save_checkpoint(checkpoint)\n",
    "        \n",
    "        #check accuracy\n",
    "        check_accuracy(val_loader, model,device=Device)\n",
    "        \n",
    "        # print some exmaples to the folder\n",
    "        save_predictions_as_imgs(val_loader,model,folder='C:/Users/clarq/Desktop/data/saved images',device=Device)\n",
    "  \n",
    "    \n",
    "   \n",
    "    print('Finished Training')  \n",
    "    \n",
    "        \n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b31f6f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# ds=T_Dataset(image_directory=train_image_directory,mask_directory=train_mask_directory,\n",
    "#                      transform=train_transform)\n",
    "# data = torch.utils.data.DataLoader(ds, batch_size=Batch_size,\n",
    "#                                    pin_memory=Pin_memory, shuffle=True,num_workers=0)\n",
    "\n",
    "\n",
    "# def imshow(img):\n",
    "# #    img = img / 2 + 0.5  # unnormalize\n",
    "#     npimg = img.numpy()\n",
    "#     plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "#     plt.show()\n",
    "\n",
    "# # get some random training images\n",
    "# dataiter = iter(data)\n",
    "# images, labels = dataiter.next()\n",
    "\n",
    "# # show images\n",
    "# imshow(torchvision.utils.make_grid(images))\n",
    "# print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36855d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''Dataloader'''\n",
    "# class SynDataset(Dataset):\n",
    "#     def __init__(self,path,transforms=None):\n",
    "#         self.image_list = listdir(path)\n",
    "#         #listdir() method in python is used to get the list of all files and directories\n",
    "#         #in the specified directory. If we don’t specify any directory, \n",
    "#         #then list of files and directories in the current working directory will be returned.\n",
    "#         self.path = path\n",
    "#         self.transforms=transforms\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.image_list)# size of image dataset \n",
    "  \n",
    "#     def __getitem__(self,idx): # index iterates from 0 to number of dataset\n",
    "#         img = np.load(self.path+self.image_list[idx])\n",
    "        \n",
    "#         im, la = img['image'], img['label']\n",
    "       \n",
    "#         sample = {'image': im, 'label': la} #make a dictionary\n",
    "    \n",
    "#         if self.transforms is not None:\n",
    "#             sample=self.transforms(sample) #在后面调用SynDataset train model 的时候会define 具体的 transform \n",
    "        \n",
    "#         return sample\n",
    "\n",
    "    \n",
    "    \n",
    "# '''data augmentation'''\n",
    "# class Resize(object):\n",
    "#     def __init__(self, output_size):\n",
    "        \n",
    "#         self.size = output_size\n",
    "      \n",
    "#     def __call__(self, sample):\n",
    "#         image, label = sample['image'], sample['label'] #call the dictionary sample['image']= im\n",
    "    \n",
    "#         im = transform.resize(image, (self.size, self.size))\n",
    "#         la = transform.resize(label, (self.size, self.size),order=0,anti_aliasing=False)\n",
    "        \n",
    "#         return {'image': im, 'label': la}\n",
    "    \n",
    "# class Rotate(object):\n",
    "\n",
    "#     def __init__(self, angle):\n",
    "        \n",
    "#         self.angle = angle\n",
    "      \n",
    "#     def __call__(self, sample):\n",
    "#         image, label = sample['image'], sample['label']\n",
    "        \n",
    "#         if np.random.rand()<0.25:        \n",
    "#             alpha = np.random.randint(-self.angle, self.angle)\n",
    "#             image = transform.rotate(image, alpha, resize=False)\n",
    "#             label = transform.rotate(label, alpha, resize=False)\n",
    "        \n",
    "#         return {'image': image, 'label': label}\n",
    "    \n",
    "# class flip(object):\n",
    "    \n",
    "#     def __call__(self, sample):\n",
    "#         image, label = sample['image'], sample['label']\n",
    "            \n",
    "#         if np.random.rand()<0.25: \n",
    "#             image = np.flipud(image).copy()\n",
    "#             label = np.flipud(label).copy()\n",
    "#         elif np.random.rand()<0.25: \n",
    "#             image = np.fliplr(image).copy()\n",
    "#             label = np.fliplr(label).copy()\n",
    "            \n",
    "#         return {'image': image, 'label': label}\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# load the train data and train the network\n",
    "# \"\"\"\n",
    "\n",
    "# train_path = '/home/dewolf151/train_npz/'\n",
    "# train_files = listdir(train_path)\n",
    "# batch_size = 24\n",
    "# num_epochs = 250 \n",
    "\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print('the available device is:', device, flush=True)\n",
    "\n",
    "           \n",
    "# composed=transforms.Compose([Resize(224),\n",
    "#                           Rotate(15),\n",
    "#                           flip()])\n",
    "\n",
    "# trainloader=DataLoader(SynDataset(train_path,composed),batch_size=batch_size,shuffle=True)\n",
    "\n",
    "# Full_Rnet = torch.hub.load('pytorch/vision:v0.6.0', 'resnet50', pretrained=True)\n",
    "# model = TransUnet(Full_Rnet)\n",
    "# model.to(device)\n",
    "\n",
    "# ce_loss = torch.nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0001)\n",
    "\n",
    "# store_loss = np.zeros((num_epochs))\n",
    "# acc = []\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#   epoch_loss = 0\n",
    "#   for ii, data in enumerate(trainloader):\n",
    "#     image, labels = data['image'].to(device), data['label'].to(device)\n",
    "    \n",
    "#     im_in = image.unsqueeze(1)\n",
    "#     prediction = model(im_in)\n",
    "    \n",
    "#     loss = ce_loss(prediction, labels[:].long())\n",
    "    \n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "    \n",
    "#     epoch_loss += loss.item()    \n",
    "        \n",
    "#   avg_loss = epoch_loss / (ii+1)\n",
    "#   print('epoch %d loss: %.3f' % (epoch + 1, avg_loss ), flush=True)\n",
    "    \n",
    "#   store_loss[epoch] = avg_loss\n",
    "  \n",
    "#   if epoch % 5 == 0:\n",
    "#       max_pred = torch.argmax(prediction,1)\n",
    "#       num_pix = labels.nelement()\n",
    "#       corr = max_pred.eq(labels).sum().item()\n",
    "#       acc.append(100 * corr / num_pix)\n",
    "      \n",
    "        \n",
    "# torch.save(model.state_dict(), 'saved_models/mod.pth')\n",
    "\n",
    "# np.save('saved_data/train_acc',acc)\n",
    "# np.save('saved_data/train_loss',store_loss)\n",
    "\n",
    "# \"\"\"\n",
    "# Define the metrics used to quantify the performance during the testing time\n",
    "# \"\"\"\n",
    "\n",
    "# def diceCoef(pred, gt):\n",
    "#     nclas = 8 #the number of classes except background\n",
    "#     N = gt.size(0) #the batch size\n",
    "    \n",
    "#     pred_flat = pred.view(N, -1)\n",
    "#     gt_flat = gt.view(N, -1)\n",
    "    \n",
    "#     Dice = np.zeros((N,nclas))\n",
    "#     for ii in range(nclas):\n",
    "#         logit_pred = pred_flat == ii+1\n",
    "#         logit_gt = gt_flat == ii+1\n",
    "     \n",
    "#         intersection = (logit_pred * logit_gt).sum(1)\n",
    "#         unionset = logit_pred.sum(1) + logit_gt.sum(1)\n",
    "#         Dice[:,ii] = 2 * (intersection) / (unionset)\n",
    "    \n",
    "#     #Dice[np.isnan(Dice)] = 0 #remove the Nan when class is not present\n",
    "#     return 100*Dice\n",
    "\n",
    "# def Hausdorff(pred, gt):\n",
    "#     batch_s = pred.size(0)\n",
    "#     HDC = np.zeros(batch_s)\n",
    "    \n",
    "#     pred[pred>0] = 1\n",
    "#     pred = pred.detach().cpu().numpy()\n",
    "    \n",
    "#     gt[gt>0] = 1\n",
    "#     gt = gt.detach().cpu().numpy()\n",
    "    \n",
    "#     for jj in range(batch_s): \n",
    "#         if pred[jj,:,:].sum() > 0 and gt[jj,:,:].sum()>0:\n",
    "#             HDC[jj] = binary.hd95(pred[jj,:,:], gt[jj,:,:])\n",
    "#         else:\n",
    "#             HDC[jj] = 0\n",
    "        \n",
    "#     return HDC\n",
    "\n",
    "# \"\"\"\n",
    "# Rescale the input images back for the test data set\n",
    "# \"\"\"\n",
    "# def Test_rescale(sample,size):\n",
    "\n",
    "#     image, label = sample['image'][:], sample['label'][:]\n",
    "    \n",
    "#     N = image.shape[0]\n",
    "#     im = transform.resize(image, (N, size, size))\n",
    "#     la = transform.resize(label, (N, size, size),order=0,preserve_range=True,anti_aliasing=False).astype(np.uint8)\n",
    "        \n",
    "#     return im, la\n",
    "\n",
    "# \"\"\"\n",
    "# Load the test data and run it through the model\n",
    "# \"\"\"\n",
    "\n",
    "# #Full_Rnet = torch.hub.load('pytorch/vision:v0.6.0', 'resnet50', pretrained=True)\n",
    "# #model = TransUnet(Full_Rnet)\n",
    "# #model.to(device)\n",
    "# #model.load_state_dict(torch.load('saved_models\\\\mod.pth'))\n",
    "\n",
    "# test_path = '/home/dewolf151/test_vol_h5/'\n",
    "# test_files = listdir(test_path)  \n",
    "\n",
    "# HD = []\n",
    "# first_it = True\n",
    "# for nn in range(len(test_files)):\n",
    "\n",
    "#     test_data = h5py.File(test_path + test_files[nn])\n",
    "    \n",
    "#     T_image, T_label = Test_rescale(test_data,224)\n",
    "    \n",
    "#     #only send the images to gpu for running it through network\n",
    "#     dataset = torch.utils.data.TensorDataset(torch.Tensor(T_image).to(device), \n",
    "#                                              torch.Tensor(T_label) )\n",
    "        \n",
    "#     testloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "#                                               shuffle=False, num_workers=0)\n",
    "    \n",
    "    \n",
    "#     for i, data in enumerate(testloader):\n",
    "#         image, labels = data\n",
    "        \n",
    "#         im_in = image.unsqueeze(1)\n",
    "#         prediction = model(im_in)\n",
    "        \n",
    "#         prediction = torch.argmax(prediction, dim=1).cpu() #get tensor back to cpu\n",
    "        \n",
    "                    \n",
    "#         if first_it:\n",
    "#             DSC = diceCoef(prediction, labels)\n",
    "#             first_it = False\n",
    "#         else:\n",
    "#             DSC = np.concatenate((DSC, diceCoef(prediction, labels)), axis=0)\n",
    "            \n",
    "#         HD = np.append(HD, Hausdorff(prediction, labels))\n",
    "        \n",
    "# np.save('saved_data/Dice_scores',DSC)\n",
    "# np.save('saved_data/HD_scores',HD)\n",
    "\n",
    "# print('training and testing has finished', flush=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7063c467",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
